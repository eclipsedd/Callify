<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <title>Callify Video Call Test (Secure) - RecordRTC Edition</title>
    <style>
      :root {
        --primary: #4f46e5;
        --primary-dark: #4338ca;
        --background: #0f172a;
        --card-bg: rgba(255, 255, 255, 0.05);
        --text-muted: rgba(255, 255, 255, 0.7);
        --accent-green: #10b981;
        --accent-red: #ef4444;
      }
    
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
    
      body {
        font-family: 'Inter', sans-serif;
        background: linear-gradient(to bottom right, var(--background), #1e3a8a);
        color: white;
        min-height: 100vh;
        padding: 20px;
      }
    
      h1 {
        text-align: center;
        color: var(--primary);
        margin-bottom: 20px;
        font-size: 2rem;
        font-weight: 700;
      }
    
      video {
        width: 45%;
        margin: 10px;
        border-radius: 12px;
        background-color: rgba(255, 255, 255, 0.05);
        border: 1px solid rgba(255, 255, 255, 0.1);
      }
    
      #videos {
        display: flex;
        justify-content: space-around;
        flex-wrap: wrap;
      }
    
      #controls,
      #recordingControls {
        margin: 20px auto;
        padding: 20px;
        background-color: var(--card-bg);
        border-radius: 12px;
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2);
        backdrop-filter: blur(10px);
        max-width: 900px;
      }
    
      #login {
        text-align: center;
        margin: 30px auto;
        max-width: 400px;
      }
    
      input[type="text"],
      select {
        padding: 10px;
        border: 1px solid rgba(255, 255, 255, 0.2);
        border-radius: 8px;
        background: rgba(255, 255, 255, 0.1);
        color: white;
        font-size: 16px;
        margin-right: 10px;
      }
    
      button {
        padding: 10px 20px;
        background-color: var(--primary);
        color: white;
        border: none;
        border-radius: 8px;
        cursor: pointer;
        font-weight: 600;
        font-size: 15px;
        transition: background-color 0.3s;
      }
    
      button:hover {
        background-color: var(--primary-dark);
      }
    
      #startCall {
        margin-left: 15px;
      }
    
      .toggle-container {
        display: flex;
        align-items: center;
        margin: 15px 0;
      }
    
      .toggle-label {
        margin-left: 10px;
      }
    
      .switch {
        position: relative;
        display: inline-block;
        width: 60px;
        height: 34px;
      }
    
      .switch input {
        opacity: 0;
        width: 0;
        height: 0;
      }
    
      .slider {
        position: absolute;
        cursor: pointer;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        background-color: #ccc;
        transition: 0.4s;
        border-radius: 34px;
      }
    
      .slider:before {
        position: absolute;
        content: "";
        height: 26px;
        width: 26px;
        left: 4px;
        bottom: 4px;
        background-color: white;
        transition: 0.4s;
        border-radius: 50%;
      }
    
      input:checked + .slider {
        background-color: var(--primary);
      }
    
      input:checked + .slider:before {
        transform: translateX(26px);
      }
    
      .emoji-reaction {
        position: fixed;
        z-index: 9999;
        font-size: 2.5rem;
        pointer-events: none;
        animation: floatUp 2s ease-out forwards;
      }
    
      @keyframes floatUp {
        0% {
          transform: translateY(0);
          opacity: 1;
        }
        100% {
          transform: translateY(-200px);
          opacity: 0;
        }
      }
    
      #spectrumCanvas {
        border: 1px solid rgba(255, 255, 255, 0.1);
        margin: 20px auto;
        display: block;
        border-radius: 8px;
        background-color: rgb(20, 20, 20);
      }
    
      .status-indicator {
        color: var(--text-muted);
        font-style: italic;
        margin-top: 10px;
        text-align: center;
      }
    
      #recordingControls button {
        margin-right: 10px;
        background-color: var(--accent-green);
      }
    
      #recordingControls button:hover {
        background-color: #059669;
      }
    
      #subtitle-box {
        margin-top: 20px;
        text-align: center;
        background: rgba(0, 0, 0, 0.7);
        color: white;
        padding: 12px 20px;
        font-size: 18px;
        border-radius: 10px;
        max-width: 80%;
        margin-left: auto;
        margin-right: auto;
        display: none;
        transition: opacity 0.3s ease;
        z-index: 999;
      }
    </style>    
    <script src="https://cdn.socket.io/4.5.4/socket.io.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fft.js/4.0.4/fft.min.js"></script>
    <script src="https://cdn.webrtc-experiment.com/RecordRTC.js"></script>
  </head>
  <body>
    <h1 style="text-align: center; color: var(--primary); font-size: 2.5rem; font-weight: 700; margin-bottom: 1.5rem;">
      📹 Callify - Video Chat
    </h1>
    
    <!-- Main Controls Section -->
    <div id="controls" style="display: block; max-width: 900px; margin: 0 auto;">
    
      <!-- Call Action Area -->
      <div id="callControls" style="display: flex; flex-direction: column; gap: 1rem; align-items: center;">
        
        <!-- Start Call Controls -->
        <div style="display: flex; gap: 10px; align-items: center;">
          <select id="userList" style="display: none;">
            <option value="">-- Select User to Call --</option>
          </select>
          <button id="startCall" style="display: none;">📞 Start Call</button>
        </div>
    
        <!-- Incoming Call Notification -->
        <div id="incomingCallControls" style="display: none; gap: 1rem;">
          <button id="acceptCall" style="background-color: var(--accent-green);">✅ Accept</button>
          <button id="rejectCall" style="background-color: var(--accent-red);">❌ Reject</button>
        </div>
    
        <!-- Active Call Controls -->
        <div id="activeCallControls" style="display: none; gap: 1rem;">
          <button id="endCall">🔚 End Call</button>
          <button id="shareScreenBtn">📤 Share Screen</button>
          <button id="pipBtn">📺 PiP Mode</button>
        </div>
      </div>
    
      <!-- Status -->
      <p class="status-indicator" id="statusIndicator">📡 Ready to make a call</p>
    
      <!-- Emoji Controls -->
      <div id="emojiControls" style="margin-top: 20px; text-align: center;">
        <span style="font-weight: 600;">🎉 React:</span>
        <button onclick="sendReaction('❤️')">❤️</button>
        <button onclick="sendReaction('😂')">😂</button>
        <button onclick="sendReaction('🔥')">🔥</button>
        <button onclick="sendReaction('🎉')">🎉</button>
        <button onclick="sendReaction('👍')">👍</button>
        <button onclick="sendReaction('😡')">😡</button>
      </div>
    
      <!-- Settings Toggles -->
      <div style="margin-top: 30px; display: flex; justify-content: center; gap: 40px; flex-wrap: wrap;">
    
        <!-- Noise Cancellation -->
        <div style="text-align: center;">
          <button id="noiseCancelToggleBtn" style="padding: 10px 16px; border-radius: 8px; font-weight: 500;">
            🎧 Noise Cancellation: <span id="noiseCancelToggleLabel">OFF</span>
          </button>
        </div>
    
        <!-- Translation Toggle -->
        <div style="text-align: center;">
          <button id="translationToggleBtn" style="padding: 10px 16px; border-radius: 8px; font-weight: 500;">
            🌍 Translate: <span id="translateStatus">OFF</span>
          </button>
        </div>
    
      </div>
    </div>
    
    <!-- Recording Controls -->
    <div id="recordingControls" style="text-align: center; margin-top: 30px;">
      <button id="startRecord">🎙️ Start Recording</button>
      <button id="stopRecord">⏹️ Stop Recording</button>
    </div>
    
    <!-- Video Area -->
    <div id="videos" style="display: flex; justify-content: space-around; flex-wrap: wrap; margin-top: 30px;">
      
      <!-- Local Video -->
       <!-- Local Video -->
      <div id="localVideoContainer" style="width: 48%; display: flex; flex-direction: column; align-items: center;">
        <video id="localVideo" autoplay muted playsinline style="width: 100%; height: auto; border-radius: 12px;"></video>
        <div style="margin-top: 10px;">
          <button id="toggleVideo">🎥 Turn Video Off</button>
          <button id="toggleMic">🎤 Mute Mic</button>
        </div>
      </div>

      <!-- Remote Video -->
      <div id="remoteVideoContainer" style="width: 48%; display: flex; flex-direction: column; align-items: center;">
        <video id="remoteVideo" autoplay playsinline style="width: 100%; height: auto; border-radius: 12px;"></video>
        <p id="remoteMicStatus" class="status-indicator" style="display: none; color: red;">🎙️ Mic is OFF</p>
        <p id="screenShareNotice" class="status-indicator" style="display: none; color: orange;">🖥️ Screen Sharing</p>
        <p id="recordingNotice" class="status-indicator" style="display: none; color: red;">🔴 Recording... <span id="recordingTime">00:00</span></p>
        <p id="callEndedMessage" class="status-indicator" style="display: none; color: darkred; font-weight: bold;">❌ Call Ended</p>
      </div>

      <!-- <div id="localVideoContainer" style="width: 48%; display: flex; flex-direction: column; align-items: center;">
        <video id="localVideo" autoplay muted style="width: 100%; height: auto; border-radius: 12px;"></video>
        <div style="margin-top: 10px;">
          <button id="toggleVideo">🎥 Turn Video Off</button>
          <button id="toggleMic">🎤 Mute Mic</button>
        </div>
      </div>
    
      <!-- Remote Video -->
      <!-- <div id="remoteVideoContainer" style="width: 48%; display: flex; flex-direction: column; align-items: center;">
        <video id="remoteVideo" autoplay style="width: 100%; height: auto; border-radius: 12px;"></video>
        <p id="remoteMicStatus" class="status-indicator" style="display: none; color: red;">🎙️ Mic is OFF</p>
        <p id="screenShareNotice" class="status-indicator" style="display: none; color: orange;">🖥️ Screen Sharing</p>
        <p id="recordingNotice" class="status-indicator" style="display: none; color: red;">🔴 Recording... <span id="recordingTime">00:00</span></p>
        <p id="callEndedMessage" class="status-indicator" style="display: none; color: darkred; font-weight: bold;">❌ Call Ended</p>
      </div> -->
    <!-- </div> --> -->
    
    <!-- Subtitle and Spectrum -->
    <div id="subtitle-box"></div>
    <canvas id="spectrumCanvas" width="600" height="100"></canvas>
    
    <script>
      let localStream;
      let rawStream;
      let peerConnection;
      let pendingCandidates = [];

      const urlParams = new URLSearchParams(window.location.search);
      const from = urlParams.get('from');
      const to = urlParams.get('to');
      const initiatorFlag = urlParams.get('initiator') === 'true';

      if (!from || !to) {
        alert("Missing from/to in URL");
        throw new Error("Missing from/to query params");
      }

      let myUsername = from;
      let targetUsername = to;
      let isInitiator = initiatorFlag; // ✅ use the flag

      const socket = io(`https://${window.location.hostname}:8443`, {
        secure: true,
        reconnection: true,
        query: { username: myUsername },
      });

      console.log('📞 Logged in as:', myUsername, '| Target:', targetUsername);

      // Get username to call from query string
      const targetFromURL = urlParams.get('target');
      if (targetFromURL) {
        targetUsername = targetFromURL;
      }
      let audioWorkletNode = null;
      let workletActive = false;
      let recorder;  // RecordRTC instance
      console.log("📍 INIT DEBUG:", { myUsername, targetUsername, isInitiator });

      const localVideo = document.getElementById('localVideo');
      const remoteVideo = document.getElementById('remoteVideo');
      console.log("📏 remoteVideo visible?", !!(remoteVideo.offsetWidth && remoteVideo.offsetHeight));
      const loginDiv = document.getElementById('login');
      const usernameInput = document.getElementById('usernameInput');
      const loginBtn = document.getElementById('loginBtn');
      const controlsDiv = document.getElementById('controls');
      const userListSelect = document.getElementById('userList');
      const startCallButton = document.getElementById('startCall');
      const noiseCancelToggle = document.getElementById('noiseCancelToggle');
      const statusIndicator = document.getElementById('statusIndicator');
      const startRecordButton = document.getElementById('startRecord');
      const stopRecordButton = document.getElementById('stopRecord');
      const acceptCallBtn = document.getElementById('acceptCall');
      const rejectCallBtn = document.getElementById('rejectCall');
      const endCallBtn = document.getElementById('endCall');
      const toggleVideoBtn = document.getElementById('toggleVideo');
      const toggleMicBtn = document.getElementById('toggleMic');
      const incomingCallControls = document.getElementById('incomingCallControls');
      const activeCallControls = document.getElementById('activeCallControls');
      const remoteMicStatus = document.getElementById('remoteMicStatus');
      const recordingNotice = document.getElementById('recordingNotice');
      const recordingTimeSpan = document.getElementById('recordingTime');
      const callEndedMessage = document.getElementById('callEndedMessage');
      let recordingInterval;
      // Noise Cancellation Button
      const noiseToggleBtn = document.getElementById('noiseCancelToggleBtn');
      const noiseLabel = document.getElementById('noiseCancelToggleLabel');

      let noiseEnabled = false;

      if (noiseToggleBtn) {
        noiseToggleBtn.addEventListener('click', async () => {
          noiseEnabled = !noiseEnabled;
          noiseLabel.textContent = noiseEnabled ? 'ON' : 'OFF';
          noiseToggleBtn.style.backgroundColor = noiseEnabled ? 'var(--accent-green)' : '';

          await getLocalMedia(); // Re-initialize stream with noise worklet
          statusIndicator.textContent = noiseEnabled
            ? '🎧 Noise Cancellation: ON'
            : 'Noise Cancellation: OFF';
        });
      }

// Translation Button
      let isTranslating = false;
      const translateBtn = document.getElementById('translationToggleBtn');
      const translateLabel = document.getElementById('translateStatus');

      if (translateBtn) {
        translateBtn.addEventListener('click', () => {
          isTranslating = !isTranslating;
          translateLabel.textContent = isTranslating ? 'ON' : 'OFF';
          translateBtn.style.backgroundColor = isTranslating ? 'var(--accent-green)' : '';

          if (isTranslating) {
            setupRemoteAudioProcessing();
            subtitleBox.style.display = 'block';
            statusIndicator.textContent = '🌍 Translation active';
          } else {
            stopRecognition();
            subtitleBox.style.display = 'none';
            statusIndicator.textContent = 'Translation turned off';
          }
        });
      }

      // Audio processing setup (noise cancellation and spectrum visualization)
      const audioContext = new AudioContext();
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 2048;
      const bufferLength = analyser.frequencyBinCount;
      const dataArray = new Uint8Array(bufferLength);

      const canvas = document.getElementById('spectrumCanvas');
      const canvasCtx = canvas.getContext('2d');

      const pipBtn = document.getElementById('pipBtn');
      
      const shareScreenBtn = document.getElementById('shareScreenBtn');
      let isScreenSharing = false;
      let originalVideoTrack = null;
      
      if (!navigator.mediaDevices.getDisplayMedia) {
        shareScreenBtn.style.display = 'none';
      }
      console.log('📥 Waiting to receive call offer via socket...');
      socket.on('call-made', async ({ callerUsername, offer }) => {
        console.log("📞 Received call from:", callerUsername);
        targetUsername = callerUsername;
        console.log("👀 Received offer:", offer.sdp.includes("m=video") ? "✅ contains video" : "❌ no video line");

        await getLocalMedia(); // must be done before setting remote description
        peerConnection = createPeerConnection();

        try {
          await peerConnection.setRemoteDescription(new RTCSessionDescription(offer));
          const answer = await peerConnection.createAnswer();
          await peerConnection.setLocalDescription(answer);

          console.log("✅ Sending answer back to caller...");
          socket.emit('make-answer', {
            toUsername: callerUsername,
            answer,
            fromUsername: myUsername
          });

          statusIndicator.textContent = 'Call established';

        } catch (err) {
          console.error('❌ Error handling call offer:', err);
        }
      });

      if (shareScreenBtn) {
        shareScreenBtn.addEventListener('click', async () => {
          if (!peerConnection) return;

          if (!isScreenSharing) {
            try {
              const screenStream = await navigator.mediaDevices.getDisplayMedia({ video: true });
              const screenTrack = screenStream.getVideoTracks()[0];
              originalVideoTrack = localStream.getVideoTracks()[0];

              // Replace the local track in the connection
              const sender = peerConnection.getSenders().find(s => s.track.kind === 'video');
              sender.replaceTrack(screenTrack);

              // Show screen locally
              localVideo.srcObject = screenStream;

              // Listen for when user stops screen share
              screenTrack.onended = () => {
                sender.replaceTrack(originalVideoTrack);
                localVideo.srcObject = localStream;
                isScreenSharing = false;
                socket.emit('screen-share-stopped', { toUsername: targetUsername });
                shareScreenBtn.textContent = '📤 Share Screen';
              };

              isScreenSharing = true;
              socket.emit('screen-share-started', { toUsername: targetUsername });
              shareScreenBtn.textContent = '🧍 Back to Camera';
            } catch (err) {
              console.error('Error sharing screen:', err);
            }
          } else {
            // Stop screen sharing manually
            const sender = peerConnection.getSenders().find(s => s.track.kind === 'video');
            if (originalVideoTrack) sender.replaceTrack(originalVideoTrack);
            localVideo.srcObject = localStream;
            isScreenSharing = false;
            shareScreenBtn.textContent = '📤 Share Screen';
          }
        });
      }

      pipBtn.addEventListener('click', async () => {
        try {
          if (document.pictureInPictureElement) {
            await document.exitPictureInPicture();
          } else {
            await remoteVideo.requestPictureInPicture();
          }
        } catch (error) {
          console.error('PiP failed:', error);
          statusIndicator.textContent = 'PiP not supported or blocked.';
        }
      });

      function drawSpectrum() {
        requestAnimationFrame(drawSpectrum);
        analyser.getByteFrequencyData(dataArray);
        canvasCtx.fillStyle = 'rgb(20, 20, 20)';
        canvasCtx.fillRect(0, 0, canvas.width, canvas.height);
        const barWidth = (canvas.width / bufferLength) * 2.5;
        let x = 0;
        for (let i = 0; i < bufferLength; i++) {
          const barHeight = dataArray[i];
          const hue = (i / bufferLength) * 360;
          canvasCtx.fillStyle = `hsl(${hue}, 100%, 50%)`;
          canvasCtx.fillRect(x, canvas.height - barHeight / 2, barWidth, barHeight / 2);
          x += barWidth + 1;
        }
      }
      drawSpectrum();

      async function setupAudioWorklet() {
        try {
          await audioContext.audioWorklet.addModule('spectral-filter-processor.js');
          console.log('Audio worklet registered successfully');
          return true;
        } catch (error) {
          console.error('Failed to setup audio worklet:', error);
          statusIndicator.textContent = 'Error setting up audio processing';
          return false;
        }
      }

      async function getLocalMedia() {
        try {
          statusIndicator.textContent = 'Setting up media...';
          rawStream = await navigator.mediaDevices.getUserMedia({
            video: { facingMode: 'user', width: { ideal: 640 }, height: { ideal: 480 } },
            audio: true
          });

          console.log('📤 Local stream tracks:', rawStream.getTracks());
          const videoTrack = rawStream.getVideoTracks()[0];
          console.log('📤 Local video track readyState:', videoTrack.readyState);

          videoTrack.enabled = true;

          await audioContext.resume();

          const source = audioContext.createMediaStreamSource(rawStream);
          const dest = audioContext.createMediaStreamDestination();
          try { analyser.disconnect(); } catch (e) {}
          if (audioWorkletNode) try { audioWorkletNode.disconnect(); } catch (e) {}

          let newAudioTrack;
          if (noiseEnabled) {
            if (!workletActive) {
              workletActive = await setupAudioWorklet();
              if (!workletActive) throw new Error('Could not initialize audio processing');
            }

            audioWorkletNode = new AudioWorkletNode(audioContext, 'spectral-filter-processor');
            audioWorkletNode.port.postMessage({ type: 'setProcessing', enabled: true });
            const cutoffNormalized = 6000 / (audioContext.sampleRate / 2);
            audioWorkletNode.port.postMessage({ type: 'setCutoff', cutoff: cutoffNormalized });

            source.connect(audioWorkletNode);
            audioWorkletNode.connect(dest);
            audioWorkletNode.connect(analyser);
            newAudioTrack = dest.stream.getAudioTracks()[0];

            console.log('🎧 Noise cancellation ON');
            statusIndicator.textContent = 'FFT-based noise cancellation active (6kHz cutoff)';
          } else {
            source.connect(dest);
            source.connect(analyser);
            newAudioTrack = dest.stream.getAudioTracks()[0];
            console.log('🎧 Noise cancellation OFF');
            statusIndicator.textContent = 'Standard audio processing';
          }

          localStream = new MediaStream([videoTrack, newAudioTrack]);
          localVideo.srcObject = localStream;
          console.log('🎥 Local video stream assigned:', localVideo.srcObject);
          console.log('✅ Final video track status:', videoTrack.enabled, videoTrack.muted);

          if (peerConnection) {
            const senders = peerConnection.getSenders();
            localStream.getTracks().forEach(track => {
              const existing = senders.find(s => s.track && s.track.kind === track.kind);
              if (existing) {
                existing.replaceTrack(track);
              } else {
                peerConnection.addTrack(track, localStream);
              }
            });
          }

        } catch (error) {
          console.error('❌ Error processing media:', error);
          statusIndicator.textContent = 'Error setting up media: ' + error.message;
        }
      }

      function createPeerConnection() {
        peerConnection = new RTCPeerConnection({
          iceServers: [
            { urls: 'stun:stun.l.google.com:19302' },
            { urls: 'turn:openrelay.metered.ca:80', username: 'openrelayproject', credential: 'openrelayproject' },
            { urls: 'turn:openrelay.metered.ca:443?transport=tcp', username: 'openrelayproject', credential: 'openrelayproject' }
          ]
        });

        console.log('🎬 Attaching', localStream.getTracks().length, 'tracks to PC');
        localStream.getTracks().forEach(track => {
          console.log(`📡 addTrack(${track.kind})`);
          peerConnection.addTrack(track, localStream);
        });
        
        peerConnection.ontrack = event => {
          const remoteStream = event.streams[0];
          if (remoteStream) {
            remoteVideo.srcObject = remoteStream;
            remoteVideo.autoplay = true;
            remoteVideo.playsInline = true;
            remoteVideo.muted = false;
            remoteVideo.controls = true;

            remoteVideo.addEventListener('dblclick', () => {
              if (remoteVideo.requestFullscreen) {
                remoteVideo.requestFullscreen();
              } else if (remoteVideo.webkitRequestFullscreen) {
                remoteVideo.webkitRequestFullscreen();
              } else if (remoteVideo.msRequestFullscreen) {
                remoteVideo.msRequestFullscreen();
              }
            });

            remoteVideo.onloadedmetadata = () => {
              remoteVideo.play().catch(err => {
                console.warn("❌ remoteVideo play() failed:", err);
                const btn = document.createElement('button');
                btn.textContent = "▶️ Click to play video";
                Object.assign(btn.style, {
                  position: 'fixed', bottom: '20px', right: '20px',
                  background: '#10b981', color: 'white', padding: '10px',
                  border: 'none', borderRadius: '8px', zIndex: 9999
                });
                btn.onclick = () => remoteVideo.play().then(() => btn.remove());
                document.body.appendChild(btn);
              });
            };
          }

        // peerConnection.ontrack = event => {
        //   const [remoteStream] = event.streams;
        //   console.log('📦 Received remote stream:', remoteStream.getTracks());
        //   remoteVideo.srcObject = remoteStream;
        //   console.log('⛳ set remoteVideo.srcObject');

        //   remoteVideo.autoplay = true;
        //   remoteVideo.playsInline = true;
        //   remoteVideo.muted = false;

        //   remoteVideo.onloadedmetadata = () => {
        //     remoteVideo.play().then(() => {
        //       console.log('▶️ remoteVideo is playing');
        //     }).catch(err => {
        //       console.warn('❌ remoteVideo play() failed:', err);
        //       const btn = document.createElement('button');
        //       btn.textContent = '▶️ Click to play video';
        //       Object.assign(btn.style, {
        //         position: 'fixed', bottom: '20px', right: '20px',
        //         background: '#10b981', color: 'white', padding: '10px',
        //         border: 'none', borderRadius: '8px', zIndex: 9999
        //       });
        //       btn.onclick = () => remoteVideo.play().then(() => btn.remove());
        //       document.body.appendChild(btn);
        //     });
        //   };

          const videoTrack = remoteStream.getVideoTracks()[0];
          if (videoTrack) {
            videoTrack.onunmute = () => {
              console.log('🎉 Remote video track is now unmuted — forcing play()');
              remoteVideo.play().catch(err => console.warn('❌ remoteVideo play() on unmute failed:', err));
            };
          }

          // const debugVideo = document.createElement('video');
          // debugVideo.srcObject = remoteStream;
          // debugVideo.autoplay = true;
          // debugVideo.controls = true;
          // debugVideo.muted = true;
          // Object.assign(debugVideo.style, {
          //   position: 'fixed', bottom: '60px', right: '20px',
          //   width: '160px', height: '120px', zIndex: 9999
          // });
          // document.body.appendChild(debugVideo);

          const checker = setInterval(() => {
            if (remoteVideo.videoWidth > 0 && remoteVideo.videoHeight > 0) {
              console.log('✅ video rendering at', remoteVideo.videoWidth, remoteVideo.videoHeight);
              clearInterval(checker);
            }
          }, 500);

          statusIndicator.textContent = '✅ Connected to peer';
        };

        peerConnection.onicecandidate = ({ candidate }) => {
          if (!candidate) return;
          if (candidate.candidate.includes('relay')) {
            console.log('🧊 TURN candidate used');
          } else if (candidate.candidate.includes('srflx')) {
            console.log('🌍 STUN-reflexive candidate used');
          } else if (candidate.candidate.includes('host')) {
            console.log('🏠 Host candidate used');
          }
          socket.emit('ice-candidate', {
            toUsername: targetUsername,
            fromUsername: myUsername,
            candidate
          });
        };

        peerConnection.oniceconnectionstatechange = () => {
          console.log('🌐 ICE state:', peerConnection.iceConnectionState);
        };

        peerConnection.onconnectionstatechange = () => {
          console.log('🔁 Conn state:', peerConnection.connectionState);
          statusIndicator.textContent =
            peerConnection.connectionState === 'connected'
              ? '🎥 Call connected'
              : '⚠️ ' + peerConnection.connectionState;
        };

        return peerConnection;
      }


      async function autoLoginAndCall() {
        if (!myUsername || !targetUsername) {
          alert("Missing user information.");
          return;
        }

        await audioContext.resume();

        statusIndicator.textContent = `Logged in as ${myUsername}`;
        if (loginDiv) loginDiv.style.display = 'none';
        controlsDiv.style.display = 'block';

        await getLocalMedia();
        peerConnection = createPeerConnection();
        startCall(); // 🧠 no need for setTimeout
      }

      window.addEventListener('DOMContentLoaded', () => {
        if (isInitiator) {
          console.log("✅ I am the initiator, auto-starting call");
          autoLoginAndCall();
        } else {
          console.log("🕓 Waiting for call offer");
          getLocalMedia(); // Just prepare media, don't start call
        }
      });

      if (noiseCancelToggle) {
        noiseCancelToggle.addEventListener('change', () => {
          getLocalMedia();
        });
      }

      socket.on('online-users', users => {
        userListSelect.innerHTML = `<option value="">-- Select User to Call --</option>`;
        for (const [username, id] of Object.entries(users)) {
          if (username !== myUsername) {
            const option = document.createElement('option');
            option.value = username;
            option.textContent = username;
            userListSelect.appendChild(option);
          }
        }
      });
      
      userListSelect.addEventListener('change', event => {
        targetUsername = event.target.value;
      });

      async function startCall() {
        console.log("📞 startCall() invoked for", myUsername, "→", targetUsername);
        callEndedMessage.style.display = 'none';
        activeCallControls.style.display = 'block';
        statusIndicator.textContent = 'Initiating call...';

        try {
          const offer = await peerConnection.createOffer();
          await peerConnection.setLocalDescription(offer);
          console.log("✅ Offer created and local description set.");
          socket.emit('call-user', {
            toUsername: targetUsername,
            offer: offer,
            fromUsername: myUsername,
          });
          peerConnection.getSenders().forEach(sender => {
            console.log(`📤 Sender: ${sender.track?.kind}, readyState: ${sender.track?.readyState}`);
          });

        } catch (error) {
          console.error('Error creating offer:', error);
          statusIndicator.textContent = 'Call failed: ' + error.message;
        }
      }

      startCallButton.addEventListener('click', () => {
        if (targetUsername) {
          startCall();
        } else {
          alert('Please select a user to call.');
        }
      });

      endCallBtn.addEventListener('click', () => {
        if (peerConnection) {
          peerConnection.close();
          peerConnection = null;
          remoteVideo.srcObject = null;
          remoteVideo.style.backgroundColor = 'black';
          callEndedMessage.style.display = 'block';
          activeCallControls.style.display = 'none';
          statusIndicator.textContent = 'Call ended';
          socket.emit('call-ended', { toUsername: targetUsername });
        }
      });


      socket.on('call-rejected', data => {
        statusIndicator.textContent = `Call rejected by ${data.rejectedBy}: ${data.reason}`;
        alert(`❌ ${data.rejectedBy} rejected your call: ${data.reason}`);
      });

      socket.on('subtitle', ({ text }) => {
        displaySubtitle(text);
      });      

      socket.on('user-offline', ({ toUsername }) => {
        statusIndicator.textContent = `${toUsername} is offline. Cannot make call.`;
        alert(`${toUsername} is currently offline.`);
      });

      socket.on('make-answer', async ({ answer, answererUsername }) => {
        activeCallControls.style.display = 'block';
        callEndedMessage.style.display = 'none';
        statusIndicator.textContent = 'Call answered by ' + answererUsername;

        try {
          await peerConnection.setRemoteDescription(new RTCSessionDescription(answer));
          while (pendingCandidates.length) {
            const candidate = pendingCandidates.shift();
            await peerConnection.addIceCandidate(new RTCIceCandidate(candidate));
          }
          console.log("✅ Remote description set from answer");
        } catch (error) {
          console.error('❌ Error setting remote description from answer:', error);
          statusIndicator.textContent = 'Call setup failed: ' + error.message;
        }
      });

      socket.on('ice-candidate', async data => {
        if (peerConnection && peerConnection.remoteDescription?.type) {
          try {
            console.log("📥 Received ICE candidate:", data.candidate);
            await peerConnection.addIceCandidate(new RTCIceCandidate(data.candidate));
          } catch (error) {
            console.error('Error adding ICE candidate', error);
          }
        } else {
          pendingCandidates.push(data.candidate);
        }
      });

      socket.on('connect', () => {
        console.log('✅ Connected as', myUsername);
        socket.emit('login', { username: myUsername });

        // if (isInitiator) {
        //   console.log("🚀 Initiator: starting call");
        //   startCall(); // ✅ Only caller starts the call
        // }
      });

      socket.on('update-mic-status', data => {
        if (
          peerConnection &&
          peerConnection.connectionState === 'connected' &&
          remoteVideo.srcObject // make sure the stream is showing
        ) {
          remoteMicStatus.style.display = data.muted ? 'block' : 'none';
        } else {
          remoteMicStatus.style.display = 'none';
        }
      });

      socket.on('recording-stopped', () => {
        stopRecordingNoticeTimer();
      });

      socket.on('call-ended', () => {
        if (remoteRecognition) {
          stopRecognition();
        }
        if (peerConnection) {
          peerConnection.close();
          peerConnection = null;
        }
        if (recorder && recorder.state === 'recording') {
          recorder.stopRecording();
        }

        remoteVideo.srcObject = null;
        remoteVideo.style.backgroundColor = 'black';
        callEndedMessage.style.display = 'block';
        activeCallControls.style.display = 'none';
        statusIndicator.textContent = 'Call ended';
        remoteMicStatus.style.display = 'none';
        if (document.pictureInPictureElement) {
          document.exitPictureInPicture().catch(() => {});
        }
      });

      socket.on('screen-share-started', () => {
        document.getElementById('screenShareNotice').style.display = 'block';
      });

      socket.on('screen-share-stopped', () => {
        document.getElementById('screenShareNotice').style.display = 'none';
      });

      socket.on('recording-started', () => {
        playBeep();
        recordingNotice.style.display = 'block';
        startRecordingNoticeTimer();
      });

      socket.on('recording-stopped', () => {
        playBeep();
        stopRecordingNoticeTimer();
      });

      function playBeep() {
        const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        const oscillator = audioCtx.createOscillator();
        const gainNode = audioCtx.createGain();

        oscillator.connect(gainNode);
        gainNode.connect(audioCtx.destination);

        oscillator.type = 'sine';
        oscillator.frequency.setValueAtTime(1000, audioCtx.currentTime); // 1000 Hz
        oscillator.start();
        oscillator.stop(audioCtx.currentTime + 0.2); // 0.2 seconds beep
      }
      
      recordingInterval = null;
      let seconds = 0;
      function startRecordingNoticeTimer() {
        seconds = 0;
        clearInterval(recordingInterval);
        recordingInterval = setInterval(() => {
          seconds++;
          const mins = String(Math.floor(seconds / 60)).padStart(2, '0');
          const secs = String(seconds % 60).padStart(2, '0');
          recordingTimeSpan.textContent = `${mins}:${secs}`;
        }, 1000);
      }

      startRecordButton.addEventListener('click', () => {
        if (!localStream || !remoteVideo.srcObject || !peerConnection) {
          statusIndicator.textContent = 'Streams not ready for recording';
          return;
        }
        playBeep();
        const oldCanvas = document.getElementById('recordingCanvas');
        if (oldCanvas) {
          oldCanvas.remove();
        }
        const canvas = document.createElement('canvas');
        canvas.id = 'recordingCanvas';
        canvas.width = 1280;
        canvas.height = 720;
        const ctx = canvas.getContext('2d');
      
        document.body.appendChild(canvas);
      
        function drawVideos() {
          ctx.drawImage(remoteVideo, 0, 0, canvas.width / 2, canvas.height);  // Left side remote
          ctx.drawImage(localVideo, canvas.width / 2, 0, canvas.width / 2, canvas.height);  // Right side local
          requestAnimationFrame(drawVideos);
        }
      
        drawVideos();
      
        const canvasStream = canvas.captureStream(30);  
      
        const audioContext = new AudioContext();
        const audioDestination = audioContext.createMediaStreamDestination();
      
        // Add local audio
        localStream.getAudioTracks().forEach(track => {
          const source = audioContext.createMediaStreamSource(new MediaStream([track]));
          source.connect(audioDestination);
        });
      
        // Add remote audio from peerConnection receivers
        peerConnection.getReceivers().forEach(receiver => {
          if (receiver.track && receiver.track.kind === 'audio') {
            const source = audioContext.createMediaStreamSource(new MediaStream([receiver.track]));
            source.connect(audioDestination);
          }
        });
      
        const finalStream = new MediaStream();
      
        canvasStream.getVideoTracks().forEach(track => finalStream.addTrack(track));
        audioDestination.stream.getAudioTracks().forEach(track => finalStream.addTrack(track));
      
        recorder = RecordRTC(finalStream, {
          type: 'video',
          mimeType: 'video/webm',
        });
      
        recorder.startRecording();
        socket.emit('recording-started', { toUsername: targetUsername });
        recordingNotice.style.display = 'block';
        startRecordingNoticeTimer();
        statusIndicator.textContent = 'Recording local + remote video & audio properly...';
        console.log('Recording started with video merged & audio mixed.');
      });   

      function stopRecordingNoticeTimer() {
        if (recordingInterval) {
          clearInterval(recordingInterval);
          recordingInterval = null;
        }
        recordingNotice.style.display = 'none';
        recordingTimeSpan.textContent = '00:00';
      }
      
      stopRecordButton.addEventListener('click', () => {
        if (recorder) {
          recorder.stopRecording(() => {
            playBeep();
            const blob = recorder.getBlob();
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.style.display = 'none';
            a.href = url;
            a.download = 'recording.webm';
            document.body.appendChild(a);
            a.click();
            URL.revokeObjectURL(url);
            statusIndicator.textContent = 'Recording saved.';
            console.log('RecordRTC recording stopped.');
            socket.emit('recording-stopped', { toUsername: targetUsername });
            stopRecordingNoticeTimer();
            const canvas = document.getElementById('recordingCanvas');
            if (canvas) canvas.remove();
          });
        }
      });

      toggleVideoBtn.addEventListener('click', () => {
        const videoTrack = localStream?.getVideoTracks()[0];
        if (videoTrack) {
          videoTrack.enabled = !videoTrack.enabled;
          toggleVideoBtn.textContent = videoTrack.enabled ? 'Turn Video Off' : 'Turn Video On';
        }
      });

      toggleMicBtn.addEventListener('click', () => {
        const audioTrack = localStream?.getAudioTracks()[0];
        if (audioTrack) {
          audioTrack.enabled = !audioTrack.enabled;
          toggleMicBtn.textContent = audioTrack.enabled ? 'Mute Mic' : 'Unmute Mic';
          if (peerConnection && peerConnection.connectionState === 'connected') {
            socket.emit('update-mic-status', {
              toUsername: targetUsername,
              muted: !audioTrack.enabled,
            });
          }
        }
      });

      window.addEventListener('beforeunload', () => {
        const endCallBtn = document.getElementById('endCallBtn');
        if (endCallBtn) {
          endCallBtn.click();
        }
      });


      // =================== New: Translation Functionality ===================
      // Variables for speech recognition and translation
// =================== Remote Audio Translation Functionality ===================
// Variables for speech recognition and translation
let remoteRecognition;
const translationToggle = document.getElementById('translationToggle');
const subtitleBox = document.getElementById('subtitle-box');

// Toggle the translation feature on/off

// Stop all recognition processes

// Translate English text to Hindi using MyMemory API
async function translateToHindi(text) {
  try {
    if (!text || text.trim() === '') return '';
    
    // Using MyMemory API as specified
    const res = await fetch(`https://api.mymemory.translated.net/get?q=${encodeURIComponent(text)}&langpair=en|hi`, {
      method: 'GET'
    });
    
    const data = await res.json();
    
    if (data.responseStatus === 200) {
      return data.responseData.translatedText;
    } else {
      throw new Error(data.responseDetails || 'Translation service error');
    }
  } catch (e) {
    console.error('Translation error:', e);
    return '[अनुवाद विफल]'; // Hindi for "Translation failed"
  }
}

// Display subtitles on screen
function displaySubtitle(text) {
  if (!text || text.trim() === '') return;
  
  subtitleBox.innerHTML = text;
  subtitleBox.style.display = 'block';
  
  clearTimeout(window.subtitleTimeout);
  window.subtitleTimeout = setTimeout(() => {
    subtitleBox.style.display = 'none';
  }, 5000); // Show for 5 seconds
}


// Setup remote audio processing to capture the other person's speech
function setupRemoteAudioProcessing() {
  // We need to wait for the remote stream to be available
  const checkForRemoteStream = setInterval(() => {
    if (remoteVideo.srcObject && remoteVideo.srcObject.getAudioTracks().length > 0) {
      clearInterval(checkForRemoteStream);
      
      try {
        // Create a new audio context for processing remote audio
        const remoteAudioContext = new AudioContext();
        const remoteSource = remoteAudioContext.createMediaStreamSource(remoteVideo.srcObject);
        const remoteDestination = remoteAudioContext.createMediaStreamDestination();
        
        remoteSource.connect(remoteDestination);
        
        // Create a speech recognition instance for the remote audio
        remoteRecognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        remoteRecognition.lang = 'en-US';
        remoteRecognition.continuous = true;
        remoteRecognition.interimResults = true;
        
        // Use a separate audio element to feed the recognition
        const remoteAudioElement = new Audio();
        remoteAudioElement.srcObject = remoteDestination.stream;
        remoteAudioElement.autoplay = true;
        document.body.appendChild(remoteAudioElement); // Needed for some browsers
        remoteAudioElement.style.display = 'none';
        
        remoteRecognition.onstart = () => {
          console.log('Remote speech recognition started');
          statusIndicator.textContent = 'Remote translation active';
        };
        
        remoteRecognition.onresult = async (event) => {
          for (let i = event.resultIndex; i < event.results.length; i++) {
            const transcript = event.results[i][0].transcript;
            
            if (event.results[i].isFinal) {
              console.log('Final remote transcript:', transcript);
              
              try {
                const translated = await translateToHindi(transcript);
                console.log('Remote translated text:', translated);
                
                // Display the translated text from remote user
                socket.emit('subtitle', { toUsername: targetUsername, text: translated });
              } catch (error) {
                console.error('Remote translation failed:', error);
                statusIndicator.textContent = 'Translation failed: ' + error.message;
              }
            }
          }
        };
        
        remoteRecognition.onerror = (event) => {
          console.error('Remote speech recognition error:', event.error);
          statusIndicator.textContent = 'Recognition error: ' + event.error;
        };
        
        remoteRecognition.onend = () => {
          console.log('Remote speech recognition ended');
          if (isTranslating) {
            // Try to restart if still active
            setTimeout(() => {
              try {
                remoteRecognition.start();
              } catch (e) {
                console.error('Failed to restart recognition:', e);
              }
            }, 1000);
          }
        };
        
        remoteRecognition.start();
        
      } catch (e) {
        console.error('Failed to setup remote audio processing:', e);
        statusIndicator.textContent = 'Failed to process remote audio: ' + e.message;
      }
    }
  }, 1000);
  
  // Safety timeout to avoid infinite checking
  setTimeout(() => clearInterval(checkForRemoteStream), 30000);
}
// Connection event handling - Reset translation when call ends
function stopRecognition() {
  if (remoteRecognition) {
    try {
      remoteRecognition.stop();
    } catch (e) {
      console.error('Error stopping recognition:', e);
    }
    remoteRecognition = null;
  }
}

function sendReaction(emoji) {
  socket.emit('reaction', { toUsername: targetUsername, emoji });
  showReaction(emoji); // show it locally too
}

socket.on('reaction', data => {
  showReaction(data.emoji); // show it when received from other user
});

// Expose for buttons
window.sendReaction = sendReaction;

function showReaction(emoji) {
  const el = document.createElement('div');
  el.className = 'emoji-reaction';
  el.textContent = emoji;
  el.style.left = Math.random() * 80 + 10 + '%'; // random horizontal position
  el.style.bottom = '20px';
  document.body.appendChild(el);
  setTimeout(() => el.remove(), 2000);
}


// Make sure the subtitle box exists and is styled properly
if (!document.getElementById('subtitle-box')) {
  const newSubtitleBox = document.createElement('div');
  newSubtitleBox.id = 'subtitle-box';
  newSubtitleBox.style.position = 'absolute';
  newSubtitleBox.style.bottom = '70px';
  newSubtitleBox.style.left = '0';
  newSubtitleBox.style.width = '100%';
  newSubtitleBox.style.textAlign = 'center';
  newSubtitleBox.style.backgroundColor = 'rgba(0, 0, 0, 0.7)';
  newSubtitleBox.style.color = 'white';
  newSubtitleBox.style.padding = '10px';
  newSubtitleBox.style.fontSize = '18px';
  newSubtitleBox.style.borderRadius = '5px';
  newSubtitleBox.style.zIndex = '1000';
  newSubtitleBox.style.display = 'none';
  
  document.getElementById('remoteVideoContainer').appendChild(newSubtitleBox);
}

// Handle end call event to clean up translation resources
endCallBtn.addEventListener('click', () => {
  if (remoteRecognition) {
  stopRecognition();
}
  // Existing end call code remains the same
});
      // =================== End Translation Functionality ===================
    </script>
  </body>
</html>
